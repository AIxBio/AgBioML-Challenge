tasks:
  overall:
    text: >
      Your goal is to build an epigenetic clock capable of achieving near-SOTA performance in the task
      of predicting chronological age from DNA methylation data. You have been provided with two files containing 
      approximately 13k DNAm samples from patients of different ages with age, tissue type, and sex labels:

      - `betas.arrow`: a feather file containing the beta values for each sample. The rows are the sample IDs and the columns are the probes IDs.
      - `metadata.arrow`: a feather file containing the metadata for each sample. The rows are the sample IDs and the columns are the metadata.

      The data comes from the 450k array. 

      To achieve success, you will need to obtain at least 0.9 Pearson correlation between your predicted ages and real ages
      in a held-out test set which I will not be providing you. The current best correlation achieved is 0.93, so this is 
      a feasible goal.
      
  understanding:
    task_1:
      text: >
        # CURRENT WORKFLOW STEP: Step 1 - Understanding the task

        Your team has been provided with the following task:

        > {overall_task_text}

        Please address the following questions:
        1. What is the purpose of this task? How does it relate to the overall goals of the aging research field?
        2. What prior studies have been done on this type of task? What are the main approaches and methods used?
        3. What are the main challenges and limitations of the prior studies?
        4. What are the most promising approaches and methods for this task?
        5. What is the nature of the data? What normalizations and transformations should be considered?
        6. How should the data be explored or visualized? What are some typical QC approaches for it?

        The next step in the workflow is Exploratory Data Analysis. So make sure to address these questions and ANY OTHERS
        that would be useful for EDA in the next step.

        Remember to work as a team, building on each other's insights. The Principal Scientist will summarize the discussion 
        and identify next steps when sufficient understanding has been reached.
        
  eda:
    subtask_1:
      text: >
        # CURRENT WORKFLOW STEP: Task 2, Subtask 1 - Exploratory Data Analysis (Team A Discussion)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions and determines when discussions should conclude
        # - Bioinformatics Expert: Specialist in omics data processing with deep knowledge of DNA methylation data handling
        # - Machine Learning Expert: Provides insights on ML architectures, feature engineering, and model evaluation approaches

        Your team has been provided with DNA methylation data and metadata files. As Team A, your task is to:

        1. Review the previous understanding phase report and identify key areas that need exploration in the data
        2. Develop a comprehensive EDA specification that includes:
           - Data quality assessment plan
           - Distribution analysis requirements
           - Feature correlation analysis needs
           - Batch effect investigation strategy
           - Visualization requirements
           - Statistical tests to perform
        3. Consider the following aspects in your specification:
           - Biological relevance of the analyses
           - Technical feasibility of implementation
           - Computational efficiency
           - Potential pitfalls and edge cases
           - Validation approaches

        Your output should be a detailed specification document that Team B can implement. It MUST include 
        STEP-BY-STEP instructions for the engineer to follow!
        The Principal Scientist will summarize the discussion and provide the final specification when the team reaches consensus.
        OF NOTE: Team B only has access to python, not R. Do NOT suggest using any R packages in your specification.

        Remember to:
        - Build on each other's expertise
        - Consider both biological and technical aspects
        - Be specific about analysis requirements
        - Justify your recommendations
        - Consider practical implementation constraints
        
    subtask_2:
      text: >
        # CURRENT WORKFLOW STEP: Task 2, Subtask 2 - Exploratory Data Analysis (Implementation)

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the technical solutions in code based on the team's plans
        # - Code Executor: Executes the code and returns the results.

        As the ML/Data Engineer, you have received the EDA specification from Team A. Your task is to:

        1. Review the specification document carefully
        2. Implement the requested analyses using the provided tools:
           - Code runner with pandas, numpy, scikit-learn, scipy, matplotlib, seaborn
           - Local file system access
        3. Create a comprehensive report that includes:
           - Code implementation details
           - Analysis results
           - Visualizations
           - Key findings
           - Potential issues or limitations
           - Recommendations for next steps

        Your report should be:
        - Well-documented with clear explanations
        - Include all relevant code and outputs
        - Highlight important patterns and insights
        - Note any technical challenges encountered
        - Suggest potential improvements or additional analyses

        Remember to:
        - Follow best practices in scientific computing
        - Include appropriate error handling
        - Optimize for performance with large datasets
        - Make code reusable and maintainable
        - Document all implementation choices
        
        
    subtask_3:
      text: >
        # CURRENT WORKFLOW STEP: Task 2, Subtask 3 - Exploratory Data Analysis (Review and Iteration)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions and determines when discussions should conclude
        # - Bioinformatics Expert: Specialist in omics data processing with deep knowledge of DNA methylation data handling
        # - Machine Learning Expert: Provides insights on ML architectures, feature engineering, and model evaluation approaches

        Team A, you have received the implementation report from the ML/Data Engineer. Your task is to:

        1. Review the implementation and results thoroughly
        2. Assess whether the specification was fully implemented
        3. Evaluate the quality and completeness of the analyses
        4. Identify any gaps or areas needing further exploration
        5. Determine if additional analyses are required

        IMPORTANT: The engineer has generated several plots that are available for your analysis. You can:
        - Use the analyze_plot tool to examine any plot mentioned in the report
        - Ask specific questions about patterns, distributions, or anomalies
        - Request additional visualizations if needed
        - Discuss the implications of the visual findings

        IMPORTANT: As a best practice, you should ALWAYS request at least one round of revision to ensure thorough analysis. Only approve the results if you are absolutely certain that no further improvements are needed.

        Based on your review, you should:
        - Acknowledge successful aspects of the implementation
        - Identify any missing or incomplete analyses
        - Suggest improvements or additional analyses if needed
        - Consider the implications of the findings for the next steps
        - Analyze and discuss the visualizations provided
        - Request additional plots if key aspects are not visualized

        If additional analyses are needed:
        - Provide a new, focused specification
        - Be specific about what needs to be done differently
        - Justify the need for additional analyses
        - Consider computational efficiency
        - Specify any additional visualizations required

        The Principal Scientist will summarize the review and either:
        - Conclude the EDA phase if satisfied by saying "APPROVE"
        - Request additional analyses if needed by saying "REVISE"

        Remember to:
        - Be constructive in your feedback
        - Consider both biological and technical aspects
        - Focus on actionable improvements
        - Maintain scientific rigor
        - Make use of the available visualizations in your analysis
        
  data_split:
    subtask_1:
      text: >
        # CURRENT WORKFLOW STEP: Task 3, Subtask 1 - Data Splitting Strategy (Team A Discussion)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions and determines when discussions should conclude
        # - Bioinformatics Expert: Specialist in omics data processing with deep knowledge of DNA methylation data handling
        # - Machine Learning Expert: Provides insights on ML architectures, feature engineering, and model evaluation approaches

        Your team has completed the Exploratory Data Analysis (EDA) phase for the DNA methylation data. Now it's time to create 
        a data splitting strategy. As Team A, your task is to:

        1. Review the findings from the EDA phase and identify key considerations for splitting the data
        2. Develop a comprehensive data splitting specification that includes:
           - Splitting ratios for train/validation/test sets
           - Stratification strategy (if any)
           - Handling of potential batch effects or confounding variables
           - Balance considerations for important variables (age distribution, tissue types, sex, etc.)
           - Evaluation criteria to ensure the splits are representative
           - Preprocessing or transformations to apply before or after splitting
        3. Consider the following aspects in your specification:
           - Biological relevance of the splitting approach
           - Technical feasibility of implementation
           - Impact on model training and evaluation
           - Potential biases and how to address them
           - Validation approaches to ensure split quality

        # CRITICAL BIOLOGICAL DATA SPLITTING GUIDELINES
        When splitting biological data, you MUST consider the following hierarchical sources of technical variance (from largest to smallest effect):

        1. Study/Dataset Level:
           - Studies represent a data collection effort, typically performed by a single research group with their own equipment, personnel, and protocols
           - Different studies have different protocols and equipment, which can introduce technical bias
           - If multiple studies/datasets exist, the test set SHOULD contain data from studies NOT represented in train/val
           - Often a major source of technical variance that can lead to overfitting
           - Example: If your data contains only E-MTAB-2372 and E-GEOD-44763, then one would be used for training/validation and the other for testing

        2. Organism/Donor Level:
           - Every human donor has a unique genome, lifestyle, and environmental exposures
           - Even inbred mice show biological variability
           - Data from the same donor should NEVER be split across train/val/test sets
           - All samples from a single donor must stay together in the same split

        3. Batch Level:
           - Represents groups of samples processed together using the same protocol and personnel (even if they are from different donors)
           - If batch information is available, consider stratifying splits by batch
           - Be careful not to confound batches with biological variables of interest

        4. Sample Level:
           - Samples with the same ID should never be split across datasets
           - Multiple measurements from the same sample must stay together in the same split

        # ⚠️ CRITICAL REQUIREMENT: DATASET-LEVEL SPLITTING ⚠️
        # You must implement a data splitting strategy that:
        #
        # 1. Identifies all unique dataset/study identifiers in the metadata
        #
        # 2. IF MULTIPLE DATASETS ARE PRESENT, separates datasets between test and train/val as follows:
        #    - NEVER select just one dataset for testing
        #    - Allocate at least 20% of total datasets to test set, when possible
        #    - Select datasets that collectively provide adequate representation across:
        #      * Tissue types (ABSOLUTE HIGHEST PRIORITY) - The test set MUST represent the tissue diversity.
        #      * Age distribution (SECOND PRIORITY)
        #      * Sex distribution (THIRD PRIORITY)
        #
        # ⚠️ FAILURE TO FOLLOW THESE EXACT PRIORITIES WILL LEAD TO REJECTION OF YOUR SOLUTION ⚠️
        # ⚠️ TEAMS MUST PRIORITIZE TISSUE TYPE REPRESENTATION ABOVE ALL OTHER CRITERIA ⚠️
        # ⚠️ A TEST SET MISSING MAJOR TISSUE TYPES IS UNACCEPTABLE ⚠️
        #
        # 3. Ensures NO overlap of datasets between test and train/val when multiple datasets exist
        #
        # 4. Provides SUFFICIENT test data (at least 20% of total samples, when possible)
        #
        # 5. For train/val splits, you do NOT need to split by dataset
        #    - Train/val can come from the same datasets
        #    - Use standard stratification approaches for train/val

        For the epigenetic clock model:
        - Consider tissue-type representation in each split, as methylation patterns vary by tissue (ABSOLUTE HIGHEST PRIORITY). The test set MUST have a representative spread of tissues.
        - Age prediction is the primary goal, so ensure age distributions are similar across splits (SECOND PRIORITY)
        - Sex is a known confounder in methylation studies and should be balanced in each split (THIRD PRIORITY)
        - Always prioritize higher-level sources of variance (study/dataset) over lower-level ones (donor/batch/sample) when making splitting decisions

        Your output should be a detailed specification document that Team B can implement. It MUST include 
        STEP-BY-STEP instructions for the engineer to follow!
        The Principal Scientist will summarize the discussion and provide the final specification when the team reaches consensus.

        Remember to:
        - Build on each other's expertise
        - Consider both biological and technical aspects
        - Be specific about your requirements and justifications
        - Ensure the splitting approach supports the goal of building an accurate epigenetic clock
        - Consider practical implementation constraints
        
    subtask_2:
      text: >
        # CURRENT WORKFLOW STEP: Task 3, Subtask 2 - Data Splitting Implementation

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the technical solutions in code based on the team's plans
        # - Code Executor: Executes the code and returns the results.

        As the ML/Data Engineer, you have received the data splitting specification from Team A. Your task is to:

        # ⚠️ CRITICAL REQUIREMENT: DATASET-LEVEL SPLITTING ⚠️
        # You must implement a data splitting strategy that:
        #
        # 1. Identifies all unique dataset/study identifiers in the metadata
        #
        # 2. IF MULTIPLE DATASETS ARE PRESENT, separates datasets between test and train/val as follows:
        #    - NEVER select just one dataset for testing
        #    - Allocate at least 20% of total datasets to test set, when possible
        #    - Select datasets that collectively provide adequate representation across:
        #      * Tissue types (ABSOLUTE HIGHEST PRIORITY) - The test set MUST represent the tissue diversity.
        #      * Age distribution (SECOND PRIORITY)
        #      * Sex distribution (THIRD PRIORITY)
        #
        # ⚠️ FAILURE TO FOLLOW THESE EXACT PRIORITIES WILL LEAD TO REJECTION OF YOUR SOLUTION ⚠️
        # ⚠️ TEAMS MUST PRIORITIZE TISSUE TYPE REPRESENTATION ABOVE ALL OTHER CRITERIA ⚠️
        # ⚠️ A TEST SET MISSING MAJOR TISSUE TYPES IS UNACCEPTABLE ⚠️
        #
        # 3. Ensures NO overlap of datasets between test and train/val when multiple datasets exist
        #
        # 4. Provides SUFFICIENT test data (at least 20% of total samples, when possible)
        #
        # 5. For train/val splits, you do NOT need to split by dataset
        #    - Train/val can come from the same datasets
        #    - Use standard stratification approaches for train/val

        1. Review the specification document carefully and execute the following ordered steps:

           a) FIRST: Identify and analyze all unique dataset/study identifiers in the metadata:
              - Count samples per dataset
              - Analyze age/sex/tissue distribution within each dataset
              - If multiple datasets exist, determine which combinations would make a representative test set
           
           b) SECOND: If multiple datasets exist, strategically select datasets for the test set that:
              - Collectively contain at least 20% of the total samples, when possible
              - Represent the overall data distribution as closely as possible
              - Cover the age range adequately (most important)
              - If only one dataset exists, use standard random splitting techniques instead
           
           c) THIRD: Allocate remaining datasets to train/validation (or split the single dataset if only one exists)
              - Split these into train and validation using standard techniques
              - Stratify by age, sex, and tissue type
           
           d) FOURTH: Verify the representativeness of all splits
              - Compare distributions across train, val, and test
              - Ensure test set is sufficiently representative

        2. Implement the requested data splitting using the provided tools:
           - Code runner with pandas, numpy, scikit-learn, scipy, matplotlib, seaborn
           - Local file system access

        3. Create the following data splits:
           - Training set
           - Validation set
           - Test set (containing samples from datasets completely separate from train/val)

        4. Generate appropriate evaluation metrics and visualizations to demonstrate:
           - The distribution of datasets across the splits (MUST show which datasets went to which split)
           - Sample counts per dataset and per split
           - The distribution of key variables (age, sex, tissue) in each split
           - Statistical tests comparing distributions between splits
           - Any identified issues or concerns

        5. Save the split datasets to the specified output files:
           - train.arrow, val.arrow, test.arrow (for the feature data)
           - train_meta.arrow, val_meta.arrow, test_meta.arrow (for the metadata)

        6. Create a comprehensive report that includes:
           - Explicit explanation of your dataset selection strategy for test
           - Justification for why your test set will provide reliable evaluation
           - Confirmation that datasets were properly separated 
           - Sample counts and percentages for each split
           - Distribution analysis of key variables across splits
           - Code implementation details
           - Visualizations of all relevant distributions
           - Any limitations of your approach

        # ⚠️ CRITICAL PLOT QUALITY REQUIREMENTS ⚠️
        # You MUST verify all visualizations for quality and accuracy:
        #
        # 1. READABILITY: All labels, legends, and text must be clearly visible and appropriately sized
        #
        # 2. ACCURACY: All data categories must be correctly represented with appropriate colors and proportions
        #
        # 3. VERIFICATION: For EACH plot:
        #    - Print a summary table showing counts and percentages that match what's in the plot
        #    - Explicitly verify the numbers match what's visually represented
        #    - If any category appears missing or incorrectly sized, recreate the plot
        #
        # 4. For gender/sex distribution plots specifically:
        #    - Always create a cross-tabulation showing counts AND percentages by split
        #    - Verify ALL genders are represented and proportionally correct
        #    - Show exact counts before and after creating the plot
        #
        # 5. For tissue type or other categorical plots:
        #    - Use horizontal orientation for better readability when many categories exist
        #    - Ensure all category labels are readable
        #    - Group small categories as "Other" if there are too many to display clearly

        # IMPORTANT VISUALIZATION NAMING REQUIREMENTS:
        - Save all plots to the output directory (task_3_workdir)
        - In your final report, include a clearly labeled "Generated Plots:" section
        - List EACH visualization filename with the .png extension (e.g., age_distribution.png)
        - The critic will search for these EXACT filenames - do not change names after listing them
        - For each filename listed, include a brief description of what the plot shows
        - Example format:
          Generated Plots:
          1. age_distribution.png - Shows age distribution across splits
          2. gender_distribution.png - Shows gender balance in each split
          3. tissue_distribution.png - Shows tissue type representation
        - Verify all plot files exist and are readable before completing your report

        # ⚠️ CRITICAL CODE EXECUTION REQUIREMENTS ⚠️
        1. ONLY USE EXECUTABLE CODE BLOCKS: 
           - ALWAYS use ```bash code blocks for executable code
           - NEVER use ```plaintext, ```json, or any other non-executable format

        2. VALID CODE EXECUTOR FORMAT:
           ```bash
           #!/bin/bash
           eval "$(conda shell.bash hook)"
           conda activate sklearn-env
           PYTHONFAULTHANDLER=1 python - <<END
           import pandas as pd
           import matplotlib.pyplot as plt
           # Your actual Python code here
           END
           ```

        3. TOOL USAGE RULES:
           - DO NOT use multi_tool_use.parallel() or any similar patterns
           - Use tools ONE AT A TIME in separate messages
           - Always wait for each tool's response before using another tool

        4. FOR ANALYZE_PLOT AND SEARCH_DIRECTORY:
           - Use these tools directly in your regular message format
           - Example: analyze_plot("task_3_workdir/age_distribution.png")
           - NOT in a code block

        5. INVALID FORMATS (DO NOT USE):
           ```plaintext
           multi_tool_use.parallel({
             "tool_uses": [...]
           })
           ```
           OR
           ```json
           {
             "tool": "analyze_plot",
             "parameters": {...}
           }
           ```

        FAILURE TO FOLLOW THESE GUIDELINES WILL CAUSE CODE EXECUTION TO FAIL.

        Your report should be:
        - Well-documented with clear explanations
        - Include all relevant code and outputs
        - Highlight important patterns and insights
        - Note any technical challenges encountered
        - Confirm that all required output files have been created

        # CRITICAL COMPLETION PROTOCOL
        # To properly conclude your task:
        # 1. Verify all output files exist and are readable using code - YOU MUST RUN A FINAL CODE BLOCK FOR THIS
        # 2. Provide a concise summary of what was accomplished
        # 3. Include a "Generated Files" section listing all output files with their purposes
        # 4. Include a "Generated Plots" section listing all created visualizations
        # 5. Follow your system prompt guidelines for properly finalizing your report


        Remember to:
        - Follow best practices in scientific computing
        - Include appropriate error handling
        - Make code reusable and maintainable
        - Document all implementation choices
        
    subtask_2_revision:
      text: >
        # CURRENT WORKFLOW STEP: Task 3, Subtask 2 - Data Splitting Implementation (REVISION)

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the technical solutions in code based on the team's plans
        # - Code Executor: Executes the code and returns the results.

        # IMPORTANT: This is ITERATION {iteration} of this subtask. You are expected to address the feedback provided by Team A in their review.

        # ⚠️ CRITICAL REQUIREMENT: DATASET-LEVEL SPLITTING ⚠️
        # If your previous implementation did not properly split by dataset/study:
        #
        # IF MULTIPLE DATASETS EXIST:
        # 1. You MUST select MULTIPLE datasets for testing
        #
        # 2. Your test datasets should:
        #    - Collectively contain at least 20%% of total samples, when possible
        #    - Represent the overall distribution with this **STRICT** priority order:
        #      * Tissue types (ABSOLUTE HIGHEST PRIORITY) - The test set MUST represent the tissue diversity.
        #      * Age distribution (SECOND PRIORITY)
        #      * Sex distribution (THIRD PRIORITY)
        #    - Have NO overlap with train/validation datasets
        #
        # ⚠️ FAILURE TO FOLLOW THESE EXACT PRIORITIES WILL LEAD TO REJECTION OF YOUR SOLUTION ⚠️
        # ⚠️ TEAMS MUST PRIORITIZE TISSUE TYPE REPRESENTATION ABOVE ALL OTHER CRITERIA ⚠️
        # ⚠️ A TEST SET MISSING MAJOR TISSUE TYPES IS UNACCEPTABLE ⚠️
        #
        # IF ONLY ONE DATASET EXISTS:
        # 3. You should use appropriate stratification techniques

        As the ML/Data Engineer, you have received FEEDBACK on your previous implementation from Team A. Your task is to:

        1. Review Team A's feedback carefully and acknowledge each point
        2. Implement the requested revisions to the data splitting approach
        3. Address all the issues and suggestions raised by the review team
        4. Enhance your previous implementation based on their guidance
        5. Create a revised comprehensive report that includes:
           - Response to each feedback point
           - Updated code implementation with proper dataset-level splitting
           - Clear tables showing sample counts per dataset and per split
           - Visualizations comparing distributions across splits
           - Statistical tests comparing distributions across splits
           - Confirmation of updated output files

        # ⚠️ CRITICAL PLOT QUALITY REQUIREMENTS ⚠️
        # You MUST verify all visualizations for quality and accuracy:
        #
        # 1. READABILITY: All labels, legends, and text must be clearly visible and appropriately sized
        #
        # 2. ACCURACY: All data categories must be correctly represented with appropriate colors and proportions
        #
        # 3. VERIFICATION: For EACH plot:
        #    - Print a summary table showing counts and percentages that match what's in the plot
        #    - Explicitly verify the numbers match what's visually represented
        #    - If any category appears missing or incorrectly sized, recreate the plot
        #
        # 4. For gender/sex distribution plots specifically:
        #    - Always create a cross-tabulation showing counts AND percentages by split
        #    - Verify ALL genders are represented and proportionally correct
        #    - Show exact counts before and after creating the plot
        #
        # 5. For tissue type or other categorical plots:
        #    - Use horizontal orientation for better readability when many categories exist
        #    - Ensure all category labels are readable
        #    - Group small categories as "Other" if there are too many to display clearly
        #
        # 6. VISUAL INSPECTION: For every plot, ask yourself these questions:
        #    - "Does this plot clearly show what I intend it to show?"
        #    - "Are all categories represented and visible?"
        #    - "Would someone unfamiliar with this data understand the visualization?"
        #    - "Do the visual proportions match the numerical statistics?"
        #    If the answer to ANY question is "no," you MUST revise the plot

        # IMPORTANT VISUALIZATION NAMING REQUIREMENTS:
        - Save all plots to the output directory (task_3_workdir)
        - In your final report, include a clearly labeled "Generated Plots:" section
        - List EACH visualization filename with the .png extension (e.g., age_distribution.png)
        - The critic will search for these EXACT filenames - do not change names after listing them
        - For each filename listed, include a brief description of what the plot shows
        - Example format:
          Generated Plots:
          1. age_distribution.png - Shows age distribution across splits
          2. gender_distribution.png - Shows gender balance in each split
          3. tissue_distribution.png - Shows tissue type representation
        - Verify all plot files exist and are readable before completing your report

        # ⚠️ CRITICAL CODE EXECUTION REQUIREMENTS ⚠️
        1. ONLY USE EXECUTABLE CODE BLOCKS: 
           - ALWAYS use ```bash code blocks for executable code
           - NEVER use ```plaintext, ```json, or any other non-executable format

        2. VALID CODE EXECUTOR FORMAT:
           ```bash
           #!/bin/bash
           eval "$(conda shell.bash hook)"
           conda activate sklearn-env
           PYTHONFAULTHANDLER=1 python - <<END
           import pandas as pd
           import matplotlib.pyplot as plt
           # Your actual Python code here
           END
           ```

        3. TOOL USAGE RULES:
           - DO NOT use multi_tool_use.parallel() or any similar patterns
           - Use tools ONE AT A TIME in separate messages
           - Always wait for each tool's response before using another tool

        4. FOR ANALYZE_PLOT AND SEARCH_DIRECTORY:
           - Use these tools directly in your regular message format
           - Example: analyze_plot("task_3_workdir/age_distribution.png")
           - NOT in a code block

        5. INVALID FORMATS (DO NOT USE):
           ```plaintext
           multi_tool_use.parallel({
             "tool_uses": [...]
           })
           ```
           OR
           ```json
           {
             "tool": "analyze_plot",
             "parameters": {...}
           }
           ```

        FAILURE TO FOLLOW THESE GUIDELINES WILL CAUSE CODE EXECUTION TO FAIL.

        Your revised report should:
        - Begin with a clear acknowledgment of the feedback points you're addressing
        - Explicitly show your dataset selection strategy for the test set
        - Provide justification for why your test set is representative
        - Include sample counts and percentages for each split
        - Show distribution analysis of key variables across splits
        - Note any ongoing challenges or limitations
        - Confirm that all required output files have been created or updated

        # CRITICAL COMPLETION PROTOCOL
        # To properly conclude your task:
        # 1. Verify all output files exist and are readable using code - YOU MUST RUN A FINAL CODE BLOCK FOR THIS
        # 2. Provide a concise summary of what was accomplished
        # 3. Include a "Generated Files" section listing all output files with their purposes
        # 4. Include a "Generated Plots" section listing all created visualizations
        # 5. Follow your system prompt guidelines for properly finalizing your report

        Remember to:
        - Maintain high-quality code with appropriate comments
        - Continue following computational efficiency guidelines
        - Ensure all visualizations are properly saved and documented
        - Address ALL the feedback points from Team A
        - Provide a complete, standalone report that incorporates the revisions

        # ⚠️ CRITICAL REPORT FORMAT REQUIREMENT ⚠️
        - Your FINAL report message MUST directly include all tabular statistics in plaintext format
        - Do NOT just print statistics during code execution and omit them from your final message
        - Copy-paste all important statistics tables directly into your final report (in markdown format) before saying the termination phrase / token.
        - The critic can ONLY evaluate statistics that appear in your final message
        - Statistics printed during code execution but omitted from your final report will be IGNORED by the critic
        
    subtask_3:
      text: >
        # CURRENT WORKFLOW STEP: Task 3, Subtask 3 - Data Splitting Review (Team A Review)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions and determines when discussions should conclude
        # - Bioinformatics Expert: Specialist in omics data processing with deep knowledge of DNA methylation data handling
        # - Machine Learning Expert: Provides insights on ML architectures, feature engineering, and model evaluation approaches

        Team A, you have received the implementation report from the ML/Data Engineer for the data splitting task. Your objective is to:

        1. Review the implementation and results thoroughly
        2. Assess whether the splits follow best practices for epigenetic datasets:
           - Dataset-level separation for test sets (if multiple datasets exist)
           - Appropriate stratification (for age, sex, and tissue type)
           - Proper verification of split quality
        3. Evaluate the evidence provided:
           - Check tabular statistics for distribution balance
           - Examine visualizations for clarity and information value
           - Verify that statistical tests confirm representativeness
        4. Determine if the implementation meets the critical requirements

        IMPORTANT: You should focus on these critical aspects:
        - The engineer MUST have properly separated datasets between test and train/val if multiple datasets exist
        - The splits should maintain similar distributions of age, sex, and tissue across all splits
        - There should be explicit statistical tests showing that splits are representative
        - Tabular statistics should clearly show the counts and percentages for key variables
        - Visualizations should effectively demonstrate the quality of the splits

        Based on your review, you should:
        - Acknowledge successful aspects of the implementation
        - Point out any issues with the data splitting approach
        - Identify any missing or inadequate evidence
        - Suggest specific improvements if needed
        - Consider the implications for downstream model training

        The Principal Scientist will summarize the review and either:
        - Accept the implementation by saying "APPROVE"
        - Request revisions if needed by saying "REVISE"

        Remember to:
        - Be thorough and specific in your feedback
        - Consider both statistical and biological validity
        - Focus on the most important issues rather than minor details
        - Make your feedback actionable for the engineer
        - Use the analyze_plot tool to examine any visualizations referenced in the report
        
  checklists:
    plot_quality: >
      # CRITICAL PLOT QUALITY CHECKLIST
      Before finalizing any data visualization, verify that each plot meets these requirements:

      1. READABILITY:
         - All axis labels are clearly visible and properly sized
         - Legend is readable and correctly represents the data
         - Title clearly explains what the plot shows
         - Text is not overlapping or cut off
         - Font sizes are consistent and appropriately sized

      2. ACCURACY:
         - Data is correctly represented (no missing categories)
         - Color scheme properly differentiates between categories
         - Proportions and distributions match the underlying data
         - No data points are accidentally excluded
         - Scales are appropriate for the data range

      3. INTERPRETABILITY:
         - The key insight is immediately apparent 
         - Comparisons between groups are clear
         - For crowded categorical plots:
           * Consider using horizontal orientation for long category names
           * Use proper spacing or faceting to avoid overcrowding
           * Ensure all categories are readable (rotate labels if needed)
         
      4. VISUAL VERIFICATION:
         - After creating each plot, you MUST:
           * Explicitly print summary statistics that confirm what the plot shows
           * Compare these statistics with what's visually represented
           * Check for any discrepancies between the numbers and the visualization
         
      5. QUALITY CONTROL STEPS:
         - For each visualization, run this verification code:
           * Print the exact count and percentage of items in each category
           * Verify these numbers match what's shown in the plot
           * If any category appears missing or incorrectly sized, recreate the plot 

  create_evaluation:
    subtask_1:
      text: >
        # CURRENT WORKFLOW STEP: Task 4, Subtask 1 - Evaluation Strategy (Team A Discussion)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions.
        # - Machine Learning Expert: Provides insights on model evaluation approaches.
        # - Bioinformatics Expert: Provides insights on epigenetic data handling and biological validation.

        The data splitting is complete. Before proceeding to model training, your team needs to define a comprehensive evaluation strategy for the epigenetic clock model. Your task is to:

        1. Review the overall project goal (Pearson R >= 0.9) and previous steps.
        2. Define the specific evaluation metrics required. 
        3. Specify the necessary visualizations. 
        4. Define the required tabular outputs (e.g., a table summarizing the key metrics).
        5. Specify the requirements for the evaluation script to be created by the engineer

        NOTE: Your ONLY task is to consider how the engineer should construct the evaluation script, not to propose modeling approaches or baselines. We will tackle that in the next step.
        NOTE: The evaluation script should be able to be used as a CLI tool in a manner like this: python evaluation_script.py --true_values path/to/true_values.csv --predicted_values path/to/predicted_values.csv

        Your output should be a detailed specification document for the engineer. The Principal Scientist will summarize the discussion and provide the final specification.

        Remember to:
        - Be specific about metric calculations and plot details (e.g., axis labels, annotations).
        - Consider edge cases or potential issues in evaluation.
        - Ensure the evaluation plan directly addresses the project's success criteria.
        
    subtask_2:
      text: >
        # CURRENT WORKFLOW STEP: Task 4, Subtask 2 - Evaluation Script Implementation

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the technical solutions.
        # - Code Executor: Executes the code.

        As the ML/Data Engineer, you have received the evaluation strategy specification from Team A. Your task is to:

        1. Review the specification document carefully.
        2. Implement a Python script (`evaluation_script.py`) that can be used as a CLI tool to perform the specified model evaluation.
           - The script MUST accept file paths for true values and predicted values as arguments
           - The script MUST use an argument parser to handle these inputs and provide a help message
           - It MUST calculate the metrics specified in the evaluation strategy
           - Evaluations should be created for the whole dataset and with respect to metadata columns (e.g. sex, tissue type, etc.)
           - It MUST generate the plots specified in the evaluation strategy
           - It MUST output the calculated metrics in a clear format (e.g., print to console or save to a results file/table).
        3. Ensure the script saves the generated plot to the designated output directory.
        4. Test the script using placeholder/dummy data (e.g., two small arrays or dataframes) to ensure it runs correctly and produces outputs in the expected format. You MUST show the output from this test run.
        5. Create a comprehensive report that includes:
           - The final code for `evaluation_script.py`.
           - Confirmation that the script runs successfully on dummy data.
           - Example output (metrics and confirmation of plot saving) from the test run.
           - Clear instructions on how to run the script with actual data files.

        # IMPORTANT REQUIREMENTS:
        - Follow all coding best practices, performance guidelines, and file saving instructions from your main system prompt.
        - Ensure the scatter plot is clear, well-labeled, and includes all required elements (identity line, annotations).
        - Verify the metrics calculations are correct.

        # CRITICAL COMPLETION PROTOCOL
        # To properly conclude your task:
        # 1. Provide the complete, final code for `evaluation_script.py`.
        # 2. Show the output from running the script on dummy data.
        # 3. Verify that the plot file was saved correctly.
        # 4. Include the standard "Generated Files" and "Generated Plots" sections.
        # 5. Follow your system prompt guidelines for finalizing your report and using the termination token.
        
  model_building:
    subtask_1:
      text: >
        # CURRENT WORKFLOW STEP: Task 5, Subtask 1 - Model Design (Team A Discussion)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader, guides decisions.
        # - ML Expert: Recommends ML architecture and training procedures.
        # - Bioinformatics Expert: Ensures biological relevance and data handling best practices.

        The data splitting and evaluation script creation stages are complete. The next step is to design the initial machine learning model for predicting chronological age from DNA methylation data.

        CRITICAL CONTEXT: The required data splits (`train.arrow`, `val.arrow`, `test.arrow` and their metadata counterparts `train_meta.arrow`, `val_meta.arrow`, `test_meta.arrow`) have ALREADY been created in the stage 3 working directory. DO NOT redo the data splitting.

        Your task is to design **EXACTLY ONE** specific ML model and training approach. Your specification must include:

        1.  **Model Architecture:** Define the precise model type (e.g., ElasticNet, RandomForestRegressor, GradientBoostingRegressor, VAE, ResNet, etc.). Specify key hyperparameters (e.g., alpha/l1_ratio for ElasticNet, number of trees/depth for RandomForest, learning rate/hidden layers for NN).
        2.  **Feature Selection/Engineering (if any):** Specify any required feature selection or transformation steps *before* training, based on EDA findings. If none, state explicitly.
        3.  **Training Procedure:** Detail how the model should be trained using the `train.arrow` and `val.arrow` data. Include:
            - Loss function.
            - Optimization algorithm (if applicable).
            - Number of epochs or training iterations (suggest a reasonable starting point).
            - Early stopping criteria (if applicable).
        4.  **Training Script Requirements (`train_model.py`):**
            - Input: Paths to training features (`train.arrow`) and training metadata (`train_meta.arrow`), validation features (`val.arrow`) and validation metadata (`val_meta.arrow`).
            - Output: A saved model file (e.g., `trained_model.pkl`, `model.h5`) in the designated output directory. Optionally, save training history/logs.
            - Functionality: Train the specified model using the defined procedure.
        5.  **Prediction Script Requirements (`predict.py`):**
            - Input: Path to a saved model file and path to input feature data (e.g., `test.arrow`).
            - Output: A predictions file (e.g., `predictions.csv` with columns `sample_id`, `predicted_age`) saved to the designated output directory.
            - Functionality: Load the specified model, generate predictions on the input data.

        Your output must be a clear, detailed specification document for the engineer. The Principal Scientist will summarize the discussion and provide the final specification.

        Remember to:
        - Focus on designing *one* well-defined model first.
        - Justify your choices based on previous findings (EDA, data splitting).
        - Be specific about hyperparameters and implementation details.
        
    subtask_2:
      text: >
        # CURRENT WORKFLOW STEP: Task 5, Subtask 2 - Model Training & Prediction Script Implementation

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the scripts.
        # - Code Executor: Executes the code.

        As the ML/Data Engineer, you have received the model design specification from Team A. Your task is to implement and test the training and prediction scripts.

        CRITICAL CONTEXT: The required data splits (`train.arrow`, `val.arrow`, `test.arrow` and their metadata counterparts `train_meta.arrow`, `val_meta.arrow`, `test_meta.arrow`) are available in the stage 3 working directory.

        Your tasks are:

        1.  **Implement Training Script (`train_model.py`):**
            - Create a Python script based *exactly* on Team A's specification.
            - Use an argument parser (`argparse`) to handle input file paths for train/val data and the output path for the saved model.
            - Implement the specified model architecture, feature handling, and training procedure.
            - Ensure the script saves the trained model to the designated output directory (`{output_dir}`).

        2.  **Test Training Script:**
            - Run `train_model.py` with the actual `train.arrow`/`train_meta.arrow` and `val.arrow`/`val_meta.arrow` files.
            - **IMPORTANT:** For this test run, limit training significantly (e.g., use only a small fraction of data, very few epochs/iterations) to ensure it completes quickly within execution limits.
            - Verify that a model file is successfully saved to the output directory.
            - Show the terminal output of this test run.

        3.  **Implement Prediction Script (`predict.py`):**
            - Create a Python script based *exactly* on Team A's specification.
            - Use `argparse` to handle input file paths for the saved model and the feature data to predict on.
            - Specify the output path for the predictions file.
            - The script should load the model saved in the previous step and generate predictions.
            - The output file (e.g., `predictions.csv`) should contain `sample_id` and `predicted_age` columns.
            - It should also save the ground-truth values in a separate file (e.g., `ground_truth.csv`) in the output directory.
            - Both prediction and ground-truth files should be ordered in the same way since they are matched by `sample_id`.
            - Both predictions and ground-truth files should contain all metadata columns

        4.  **Test Prediction Script:**
            - Run `predict.py` using the model saved during the training test run.
            - Use a small subset of the validation data (`val.arrow`) or dummy data as input for this test.
            - Verify that a predictions file is successfully created in the output directory.
            - Show the terminal output and the first few lines (`head()`) of the generated predictions file.

        5.  **Create Comprehensive Report:** Include:
            - Final code for `train_model.py`.
            - Final code for `predict.py`.
            - Confirmation and output logs from the *test run* of `train_model.py`.
            - Confirmation and output (including `head()` of predictions) from the *test run* of `predict.py`.
            - Clear instructions on how to run both scripts for full training and prediction.

        # IMPORTANT REQUIREMENTS:
        - Adhere strictly to Team A's model specification.
        - Follow all coding best practices, performance guidelines, file saving instructions, and script writing templates (using template #2 for saving scripts) from your main system prompt.
        - Ensure model saving/loading works correctly.

        # CRITICAL COMPLETION PROTOCOL
        # To properly conclude your task:
        # 1. Provide the complete, final code for `train_model.py`.
        # 2. Provide the complete, final code for `predict.py`.
        # 3. Show output from test runs of both scripts.
        # 4. Verify saved model and prediction files were created and have the correct number of rows and headers.
        # 5. Include the standard "Generated Files" section listing `train_model.py`, `predict.py`, the saved model file, and the test prediction file.
        # 6. Follow your system prompt guidelines for finalizing your report and using the termination token.
        
    # Note: No subtask_2_revision needed for model building initially, 
    # Revisions will happen based on evaluation results in later stages.
        
  train_evaluate:
    subtask_1:
      text: >
        # CURRENT WORKFLOW STEP: Task 6, Subtask 1 - Training & Evaluation Planning (Team A Discussion)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader, guides decisions.
        # - ML Expert: Advises on training finalization and evaluation interpretation.
        # - Bioinformatics Expert: Considers biological plausibility of results.

        The initial model design, training script (`train_model.py`), prediction script (`predict.py`), and evaluation script (`evaluation_script.py`) are complete. Now, we need to plan the full model training run and how evaluation will be performed.

        CRITICAL CONTEXT:
        - Data Splits: `train.arrow`, `val.arrow`, `test.arrow` (and metadata) are in the stage 3 workdir (`task_3_workdir`).
        - Scripts: `train_model.py`, `predict.py` are in the stage 5 output dir (`task_5_workdir`).
        - Evaluation Script: `evaluation_script.py` is in the stage 4 output dir (`task_4_workdir`).

        Your task is to specify the execution plan for the engineer:

        1.  **Final Training Configuration:** Define the exact parameters for the *full* run of `train_model.py` (e.g., number of epochs, batch size, fraction of data to use - likely 1.0 for final training).
        2.  **Script Modifications (if any):** Specify if any changes are needed to `train_model.py` or `predict.py` before the final run. If none, state explicitly.
        3.  **Prediction Runs:** Specify which datasets (`val.arrow`, `test.arrow`, or both) the `predict.py` script should be run on using the final trained model.
        4.  **Evaluation Execution:** Detail how and when `evaluation_script.py` should be used:
            - Run on predictions from which dataset(s)? (`val`, `test`?)
            - Should evaluation happen during training (e.g., using validation data each epoch) or only after training completes? If during training, how should `train_model.py` be modified to incorporate this?
            - Specify the final expected evaluation outputs (metrics file/table, plots).
        5.  **Expected Outputs:** List all files expected at the end of the next subtask (e.g., final saved model, final prediction file(s), final evaluation report/plots).

        Your output must be a clear, step-by-step execution plan for the engineer. The Principal Scientist will summarize the discussion and provide the final specification.

        Remember to:
        - Be precise about execution commands and parameters.
        - Clearly state which datasets to use for prediction and evaluation.
        
    subtask_2:
      text: >
        # CURRENT WORKFLOW STEP: Task 6, Subtask 2 - Execute Training & Evaluation

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Executes the plan.
        # - Code Executor: Executes the code.

        As the ML/Data Engineer, you have received the final training and evaluation plan from Team A.

        CRITICAL CONTEXT:
        - Data Splits: Located in `task_3_workdir/`
        - Training/Prediction Scripts: Located in `task_5_workdir/`
        - Evaluation Script: Located in `task_4_workdir/`
        - Your Current Output Directory: `{output_dir}`

        Your task is to execute the plan precisely:

        1.  **Copy Scripts:** Copy `train_model.py`, `predict.py` from `task_5_workdir` and `evaluation_script.py` from `task_4_workdir` into your current working directory (`{workdir}`).
        2.  **Modify Scripts (if specified):** Implement any minor modifications to the copied scripts as detailed in Team A's plan.
        3.  **Execute Full Training:** Run the `train_model.py` script using the exact configuration (epochs, data paths, etc.) specified by Team A. Ensure the final trained model is saved to `{output_dir}`.
            - **IMPORTANT:** Use the full paths to the data in `task_3_workdir` when calling the script.
            - Monitor execution time. This may take longer than previous test runs.
        4.  **Execute Prediction:** Run the `predict.py` script using the final trained model (from `{output_dir}`) on the dataset(s) specified by Team A (e.g., `task_3_workdir/val.arrow`, `task_3_workdir/test.arrow`). Ensure prediction files are saved to `{output_dir}`.
        5.  **Execute Evaluation:** Run the `evaluation_script.py` using the prediction file(s) and corresponding ground truth file(s) as specified. Ensure evaluation outputs (metrics, plots) are saved to `{output_dir}`.
        6.  **Create Comprehensive Report:** Include:
            - Confirmation that scripts were copied and modified (if applicable).
            - The exact command used to run `train_model.py` and confirmation of completion/model saving.
            - The exact command(s) used to run `predict.py` and confirmation of prediction file generation.
            - The exact command(s) used to run `evaluation_script.py`.
            - The final evaluation metrics reported by the script.
            - List all key generated files (final model, prediction file(s), evaluation plot(s), metrics file).

        # IMPORTANT REQUIREMENTS:
        - Follow Team A's execution plan EXACTLY.
        - Use full paths when referencing files in other task directories.
        - Ensure all specified output files are generated in `{output_dir}`.
        - Follow all coding best practices, performance guidelines, and file saving instructions from your main system prompt.

        # CRITICAL COMPLETION PROTOCOL
        # To properly conclude your task:
        # 1. Confirm successful execution of all steps.
        # 2. Report the final evaluation metrics.
        # 3. Include the standard "Generated Files" and "Generated Plots" sections, listing all final outputs saved in `{output_dir}`.
        # 4. Follow your system prompt guidelines for finalizing your report and using the termination token.

    # Note: No revision task needed here typically. Iteration on model performance would likely loop back to Stage 5 (Model Building) or require a new strategy.
        
  review_iterate:
    subtask_1:
      text: >
        # CURRENT WORKFLOW STEP: Task 7, Subtask 1 - Review Results & Plan Next Iteration (Team A Discussion)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader, guides decisions, synthesizes findings.
        # - ML Expert: Analyzes model performance, suggests improvements.
        # - Bioinformatics Expert: Assesses biological plausibility, suggests feature/data handling improvements.

        The first full model training and evaluation cycle (Stage 6) is complete. Your team needs to critically review the results and plan the next iteration to improve the epigenetic clock's performance towards the goal (Pearson R >= 0.9).

        CRITICAL CONTEXT:
        - Review the final evaluation report from Stage 6 (metrics, plots).
        - Consider the model design from Stage 5 and the training process from Stage 6.
        - YOU MUST USE perplexity_search AT LEAST ONCE.

        Your task is to:

        1.  **Analyze Evaluation Results:** Discuss the performance metrics (Pearson R, MAE, RMSE) and visualizations (scatter plot, error plots) from Stage 6. Identify strengths and weaknesses of the current model.
        2.  **Diagnose Performance Issues:** Hypothesize reasons for performance limitations (e.g., underfitting, overfitting, poor feature representation, suboptimal hyperparameters, issues highlighted by specific tissue types or age groups).
        3.  Review literature on epigenetic clocks to find potential improvements. YOU MUST USE perplexity_search AT LEAST ONCE.
        4.  **Propose Improvements:** Brainstorm and decide on specific, actionable changes for the next iteration. This could involve:
            - Modifying hyperparameters of the *existing* model.
            - Adjusting feature selection/engineering.
            - Trying a *different* model architecture.
            - Modifying the training procedure (e.g., learning rate schedule, data augmentation if applicable).
        5.  **Define Next Specification:** Create a clear specification for the engineer for the *next* round. This specification should detail:
            - **EITHER:** Changes to the existing `train_model.py` / `predict.py` scripts (if modifying the previous model/training). Specify exactly what needs to change.
            - **OR:** A complete design for a *new* model (if trying a different architecture), following the same structure as the Stage 5, Subtask 1 specification.
            - Specify the parameters for the next run of `train_model.py` and the subsequent prediction/evaluation steps.

        Your output must be a detailed specification document outlining the analysis of the previous results and the plan for the next modeling iteration. The Principal Scientist will summarize the discussion and provide the final specification.

        Remember to:
        - Base your decisions on the evidence from the evaluation results.
        - Be specific and justify your proposed changes.
        - Clearly define the goal for the next iteration.
        - Research and propose improvments based on the literature (remember to use perplexity to check your sources)
        
        IMPORTANT RULES:
          - The engineer only has access to datasets which they can obtain programmatically.
          - Do NOT request that they curate novel datasets (e.g., from the supplementary files of published papers).
          - Do NOT request that they use any dataset which is not publicly available.
      
    subtask_2:
      text: >
        # CURRENT WORKFLOW STEP: Task 7, Subtask 2 - Implement & Evaluate Iteration {iteration}

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the changes and executes runs.
        # - Code Executor: Executes the code.

        As the ML/Data Engineer, you have received the specification for the next modeling iteration from Team A.

        CRITICAL CONTEXT - LOCATIONS OF INPUTS:
        - Data Splits (Stage 3): `./task_3_workdir/`
        - Previous Model/Prediction Scripts (Stage 5): `./task_5_workdir/`
        - Evaluation Script (Stage 4): `./task_4_workdir/`
        - Previous Trained Model (Stage 6 or previous Iteration): `./task_6_workdir/` (or previous iteration's output dir)
        - Your Current Output Directory for this iteration: `./{output_dir}`

        Your task is to implement the specified changes and re-run the training/evaluation WITH ALL OF TRAIN, VAL, AND TEST DATA:

        1.  **Copy Base Scripts:** Copy necessary scripts (`train_model.py`, `predict.py`, `evaluation_script.py`) from their respective stage directories (`task_5_workdir`, `task_4_workdir`) into your current working directory (`{workdir}`).
        2.  **Implement Changes:** Modify the copied scripts (`train_model.py`, `predict.py`) according to the **EXACT** specification from Team A (Subtask 1 of this stage).
            - If a *new* model architecture was specified, implement it fully.
            - If *modifications* were specified, apply them carefully.
        3.  **Execute Training:** Run the modified `train_model.py` using the configuration parameters specified by Team A for this iteration. Save the newly trained model to `{output_dir}`.
            - Use the full paths to the data in `task_3_workdir`.
        4.  **Execute Prediction:** Run the modified `predict.py` using the newly trained model on the dataset(s) specified by Team A. Save prediction file(s) to `{output_dir}`.
        5.  **Execute Evaluation:** Run `evaluation_script.py` on the new prediction file(s) and ground truth data. Save evaluation outputs (metrics, plots) to `{output_dir}`.
        6.  **Create Comprehensive Report:** Include:
            - Summary of changes implemented based on Team A's specification.
            - Confirmation that scripts were copied/modified.
            - Exact commands used for training, prediction, and evaluation.
            - Final evaluation metrics for this iteration.
            - List of all key generated files for this iteration (model, predictions, plots, metrics).

        # IMPORTANT REQUIREMENTS:
        - Follow Team A's specification precisely.
        - Ensure all scripts run correctly after modification.
        - Save all outputs for this iteration to `{output_dir}`.
        - Ensure to run evaluations on the testing dataset before concluding the iteration.
        - Use computational resources efficiently. Recall you have access to 220GB of RAM and an A10 GPU with 24GB of VRAM.
        - Follow all coding best practices, performance guidelines, and file saving instructions.

        # CRITICAL COMPLETION PROTOCOL:
        # 1. Confirm successful execution of all steps.
        # 2. Report the final evaluation metrics for THIS iteration.
        # 3. Include the standard "Generated Files" and "Generated Plots" sections for THIS iteration's outputs.
        # 4. Follow your system prompt guidelines for finalizing your report and using the termination token.
        