tasks:
  overall:
    text: >
      Your goal is to build an epigenetic clock capable of achieving near-SOTA performance in the task
      of predicting chronological age from DNA methylation data. You have been provided with two files containing 
      approximately 13k DNAm samples from patients of different ages with age, tissue type, and sex labels:

      - `betas.arrow`: a feather file containing the beta values for each sample. The rows are the sample IDs and the columns are the probes IDs.
      - `metadata.arrow`: a feather file containing the metadata for each sample. The rows are the sample IDs and the columns are the metadata.

      The data comes from the 450k array. 

      To achieve success, you will need to obtain at least 0.9 Pearson correlation between your predicted ages and real ages
      in a held-out test set which I will not be providing you. The current best correlation achieved is 0.93, so this is 
      a feasible goal.
      
  understanding:
    task_1:
      text: >
        # CURRENT WORKFLOW STEP: Step 1 - Understanding the task

        Your team has been provided with the following task:

        > {overall_task_text}

        Please address the following questions:
        1. What is the purpose of this task? How does it relate to the overall goals of the aging research field?
        2. What prior studies have been done on this type of task? What are the main approaches and methods used?
        3. What are the main challenges and limitations of the prior studies?
        4. What are the most promising approaches and methods for this task?
        5. What is the nature of the data? What normalizations and transformations should be considered?
        6. How should the data be explored or visualized? What are some typical QC approaches for it?

        The next step in the workflow is Exploratory Data Analysis. So make sure to address these questions and ANY OTHERS
        that would be useful for EDA in the next step.

        Remember to work as a team, building on each other's insights. The Principal Scientist will summarize the discussion 
        and identify next steps when sufficient understanding has been reached.
        
  eda:
    subtask_1:
      text: >
        # CURRENT WORKFLOW STEP: Task 2, Subtask 1 - Exploratory Data Analysis (Team A Discussion)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions and determines when discussions should conclude
        # - Bioinformatics Expert: Specialist in omics data processing with deep knowledge of DNA methylation data handling
        # - Machine Learning Expert: Provides insights on ML architectures, feature engineering, and model evaluation approaches

        Your team has been provided with DNA methylation data and metadata files. As Team A, your task is to:

        1. Review the previous understanding phase report and identify key areas that need exploration in the data
        2. Develop a comprehensive EDA specification that includes:
           - Data quality assessment plan
           - Distribution analysis requirements
           - Feature correlation analysis needs
           - Batch effect investigation strategy
           - Visualization requirements
           - Statistical tests to perform
        3. Consider the following aspects in your specification:
           - Biological relevance of the analyses
           - Technical feasibility of implementation
           - Computational efficiency
           - Potential pitfalls and edge cases
           - Validation approaches

        Your output should be a detailed specification document that Team B can implement. It MUST include 
        STEP-BY-STEP instructions for the engineer to follow!
        The Principal Scientist will summarize the discussion and provide the final specification when the team reaches consensus.
        OF NOTE: Team B only has access to python, not R. Do NOT suggest using any R packages in your specification.

        Remember to:
        - Build on each other's expertise
        - Consider both biological and technical aspects
        - Be specific about analysis requirements
        - Justify your recommendations
        - Consider practical implementation constraints
        
    subtask_2:
      text: >
        # CURRENT WORKFLOW STEP: Task 2, Subtask 2 - Exploratory Data Analysis (Implementation)

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the technical solutions in code based on the team's plans
        # - Code Executor: Executes the code and returns the results.

        As the ML/Data Engineer, you have received the EDA specification from Team A. Your task is to:

        1. Review the specification document carefully
        2. Implement the requested analyses using the provided tools:
           - Code runner with pandas, numpy, scikit-learn, scipy, matplotlib, seaborn
           - Local file system access
        3. Create a comprehensive report that includes:
           - Code implementation details
           - Analysis results
           - Visualizations
           - Key findings
           - Potential issues or limitations
           - Recommendations for next steps

        Your report should be:
        - Well-documented with clear explanations
        - Include all relevant code and outputs
        - Highlight important patterns and insights
        - Note any technical challenges encountered
        - Suggest potential improvements or additional analyses

        Remember to:
        - Follow best practices in scientific computing
        - Include appropriate error handling
        - Optimize for performance with large datasets
        - Make code reusable and maintainable
        - Document all implementation choices
        
    subtask_2_revision:
      text: >
        # CURRENT WORKFLOW STEP: Task 2, Subtask 2 - Exploratory Data Analysis (REVISION)

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the technical solutions in code based on the team's plans
        # - Code Executor: Executes the code and returns the results.

        # IMPORTANT: This is ITERATION {iteration} of this subtask. You are expected to address the feedback provided by Team A in their review.

        As the ML/Data Engineer, you have received FEEDBACK on your previous implementation from Team A. Your task is to:

        1. Review Team A's feedback carefully and acknowledge each point
        2. Implement the requested revisions and additional analyses
        3. Address all the issues and suggestions raised by the review team
        4. Enhance your previous implementation based on their guidance
        5. Create a revised comprehensive report that includes:
           - Response to each feedback point
           - Updated code implementation
           - New or enhanced visualizations
           - Additional analyses requested
           - Revised findings and insights

        Your revised report should:
        - Begin with a clear acknowledgment of the feedback points you're addressing
        - Explicitly reference how each implementation change responds to the feedback
        - Include improved visualizations and analyses as requested
        - Highlight new insights discovered through the revisions
        - Note any ongoing challenges or limitations

        Remember to:
        - Maintain high-quality code with appropriate comments
        - Continue following computational efficiency guidelines
        - Ensure all visualizations are properly saved and documented
        - Address ALL the feedback points from Team A
        - Provide a complete, standalone report that incorporates the revisions
        
    subtask_3:
      text: >
        # CURRENT WORKFLOW STEP: Task 2, Subtask 3 - Exploratory Data Analysis (Review and Iteration)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions and determines when discussions should conclude
        # - Bioinformatics Expert: Specialist in omics data processing with deep knowledge of DNA methylation data handling
        # - Machine Learning Expert: Provides insights on ML architectures, feature engineering, and model evaluation approaches

        Team A, you have received the implementation report from the ML/Data Engineer. Your task is to:

        1. Review the implementation and results thoroughly
        2. Assess whether the specification was fully implemented
        3. Evaluate the quality and completeness of the analyses
        4. Identify any gaps or areas needing further exploration
        5. Determine if additional analyses are required

        IMPORTANT: The engineer has generated several plots that are available for your analysis. You can:
        - Use the analyze_plot tool to examine any plot mentioned in the report
        - Ask specific questions about patterns, distributions, or anomalies
        - Request additional visualizations if needed
        - Discuss the implications of the visual findings

        IMPORTANT: As a best practice, you should ALWAYS request at least one round of revision to ensure thorough analysis. Only approve the results if you are absolutely certain that no further improvements are needed.

        Based on your review, you should:
        - Acknowledge successful aspects of the implementation
        - Identify any missing or incomplete analyses
        - Suggest improvements or additional analyses if needed
        - Consider the implications of the findings for the next steps
        - Analyze and discuss the visualizations provided
        - Request additional plots if key aspects are not visualized

        If additional analyses are needed:
        - Provide a new, focused specification
        - Be specific about what needs to be done differently
        - Justify the need for additional analyses
        - Consider computational efficiency
        - Specify any additional visualizations required

        The Principal Scientist will summarize the review and either:
        - Conclude the EDA phase if satisfied by saying "APPROVE"
        - Request additional analyses if needed by saying "REVISE"

        Remember to:
        - Be constructive in your feedback
        - Consider both biological and technical aspects
        - Focus on actionable improvements
        - Maintain scientific rigor
        - Make use of the available visualizations in your analysis
        
  data_split:
    subtask_1:
      text: >
        # CURRENT WORKFLOW STEP: Task 3, Subtask 1 - Data Splitting Strategy (Team A Discussion)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions and determines when discussions should conclude
        # - Bioinformatics Expert: Specialist in omics data processing with deep knowledge of DNA methylation data handling
        # - Machine Learning Expert: Provides insights on ML architectures, feature engineering, and model evaluation approaches

        Your team has completed the Exploratory Data Analysis (EDA) phase for the DNA methylation data. Now it's time to create 
        a data splitting strategy. As Team A, your task is to:

        1. Review the findings from the EDA phase and identify key considerations for splitting the data
        2. Develop a comprehensive data splitting specification that includes:
           - Splitting ratios for train/validation/test sets
           - Stratification strategy (if any)
           - Handling of potential batch effects or confounding variables
           - Balance considerations for important variables (age distribution, tissue types, sex, etc.)
           - Evaluation criteria to ensure the splits are representative
           - Preprocessing or transformations to apply before or after splitting
        3. Consider the following aspects in your specification:
           - Biological relevance of the splitting approach
           - Technical feasibility of implementation
           - Impact on model training and evaluation
           - Potential biases and how to address them
           - Validation approaches to ensure split quality

        # CRITICAL BIOLOGICAL DATA SPLITTING GUIDELINES
        When splitting biological data, you MUST consider the following hierarchical sources of technical variance (from largest to smallest effect):

        1. Study/Dataset Level:
           - Studies represent a data collection effort, typically performed by a single research group with their own equipment, personnel, and protocols
           - Different studies have different protocols and equipment, which can introduce technical bias
           - If multiple studies/datasets exist, the test set SHOULD contain data from studies NOT represented in train/val
           - Often a major source of technical variance that can lead to overfitting
           - Example: If your data contains only E-MTAB-2372 and E-GEOD-44763, then one would be used for training/validation and the other for testing

        2. Organism/Donor Level:
           - Every human donor has a unique genome, lifestyle, and environmental exposures
           - Even inbred mice show biological variability
           - Data from the same donor should NEVER be split across train/val/test sets
           - All samples from a single donor must stay together in the same split

        3. Batch Level:
           - Represents groups of samples processed together using the same protocol and personnel (even if they are from different donors)
           - If batch information is available, consider stratifying splits by batch
           - Be careful not to confound batches with biological variables of interest

        4. Sample Level:
           - Samples with the same ID should never be split across datasets
           - Multiple measurements from the same sample must stay together in the same split

        # ⚠️ CRITICAL REQUIREMENT: DATASET-LEVEL SPLITTING ⚠️
        # You must implement a data splitting strategy that:
        #
        # 1. Identifies all unique dataset/study identifiers in the metadata
        #
        # 2. IF MULTIPLE DATASETS ARE PRESENT, separates datasets between test and train/val as follows:
        #    - NEVER select just one dataset for testing
        #    - Allocate at least 33% of total datasets to test set, when possible
        #    - Select datasets that collectively provide adequate representation across:
        #      * Age distribution (priority #1)
        #      * Sex distribution (priority #2)
        #      * Tissue types (priority #3)
        #
        # 3. Ensures NO overlap of datasets between test and train/val when multiple datasets exist
        #
        # 4. Provides SUFFICIENT test data (at least 33% of total samples, when possible)
        #
        # 5. For train/val splits, you do NOT need to split by dataset
        #    - Train/val can come from the same datasets
        #    - Use standard stratification approaches for train/val

        For the epigenetic clock model:
        - Age prediction is the primary goal, so ensure age distributions are similar across splits
        - Consider tissue-type representation in each split, as methylation patterns vary by tissue. 
                - Even if it's not possible to split perfectly, try to ensure that the splits are as balanced as possible.
        - Sex is a known confounder in methylation studies and should be balanced in each split
        - Always prioritize higher-level sources of variance (study/dataset) over lower-level ones (donor/batch/sample) when making splitting decisions

        Your output should be a detailed specification document that Team B can implement. It MUST include 
        STEP-BY-STEP instructions for the engineer to follow!
        The Principal Scientist will summarize the discussion and provide the final specification when the team reaches consensus.

        Remember to:
        - Build on each other's expertise
        - Consider both biological and technical aspects
        - Be specific about your requirements and justifications
        - Ensure the splitting approach supports the goal of building an accurate epigenetic clock
        - Consider practical implementation constraints
        
    subtask_2:
      text: >
        # CURRENT WORKFLOW STEP: Task 3, Subtask 2 - Data Splitting Implementation

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the technical solutions in code based on the team's plans
        # - Code Executor: Executes the code and returns the results.

        As the ML/Data Engineer, you have received the data splitting specification from Team A. Your task is to:

        # ⚠️ CRITICAL REQUIREMENT: DATASET-LEVEL SPLITTING ⚠️
        # You must implement a data splitting strategy that:
        #
        # 1. Identifies all unique dataset/study identifiers in the metadata
        #
        # 2. IF MULTIPLE DATASETS ARE PRESENT, separates datasets between test and train/val as follows:
        #    - NEVER select just one dataset for testing
        #    - Allocate at least 33% of total datasets to test set, when possible
        #    - Select datasets that collectively provide adequate representation across:
        #      * Age distribution (priority #1)
        #      * Sex distribution (priority #2)
        #      * Tissue types (priority #3)
        #
        # 3. Ensures NO overlap of datasets between test and train/val when multiple datasets exist
        #
        # 4. Provides SUFFICIENT test data (at least 33% of total samples, when possible)
        #
        # 5. For train/val splits, you do NOT need to split by dataset
        #    - Train/val can come from the same datasets
        #    - Use standard stratification approaches for train/val

        1. Review the specification document carefully and execute the following ordered steps:

           a) FIRST: Identify and analyze all unique dataset/study identifiers in the metadata:
              - Count samples per dataset
              - Analyze age/sex/tissue distribution within each dataset
              - If multiple datasets exist, determine which combinations would make a representative test set
           
           b) SECOND: If multiple datasets exist, strategically select datasets for the test set that:
              - Collectively contain at least 33% of the total samples, when possible
              - Represent the overall data distribution as closely as possible
              - Cover the age range adequately (most important)
              - If only one dataset exists, use standard random splitting techniques instead
           
           c) THIRD: Allocate remaining datasets to train/validation (or split the single dataset if only one exists)
              - Split these into train and validation using standard techniques
              - Stratify by age, sex, and tissue type
           
           d) FOURTH: Verify the representativeness of all splits
              - Compare distributions across train, val, and test
              - Ensure test set is sufficiently representative

        2. Implement the requested data splitting using the provided tools:
           - Code runner with pandas, numpy, scikit-learn, scipy, matplotlib, seaborn
           - Local file system access

        3. Create the following data splits:
           - Training set
           - Validation set
           - Test set (containing samples from datasets completely separate from train/val)

        4. Generate appropriate evaluation metrics and visualizations to demonstrate:
           - The distribution of datasets across the splits (MUST show which datasets went to which split)
           - Sample counts per dataset and per split
           - The distribution of key variables (age, sex, tissue) in each split
           - Statistical tests comparing distributions between splits
           - Any identified issues or concerns

        5. Save the split datasets to the specified output files:
           - train.arrow, val.arrow, test.arrow (for the feature data)
           - train_meta.arrow, val_meta.arrow, test_meta.arrow (for the metadata)

        6. Create a comprehensive report that includes:
           - Explicit explanation of your dataset selection strategy for test
           - Justification for why your test set will provide reliable evaluation
           - Confirmation that datasets were properly separated 
           - Sample counts and percentages for each split
           - Distribution analysis of key variables across splits
           - Code implementation details
           - Visualizations of all relevant distributions
           - Any limitations of your approach

        # ⚠️ CRITICAL PLOT QUALITY REQUIREMENTS ⚠️
        # You MUST verify all visualizations for quality and accuracy:
        #
        # 1. READABILITY: All labels, legends, and text must be clearly visible and appropriately sized
        #
        # 2. ACCURACY: All data categories must be correctly represented with appropriate colors and proportions
        #
        # 3. VERIFICATION: For EACH plot:
        #    - Print a summary table showing counts and percentages that match what's in the plot
        #    - Explicitly verify the numbers match what's visually represented
        #    - If any category appears missing or incorrectly sized, recreate the plot
        #
        # 4. For gender/sex distribution plots specifically:
        #    - Always create a cross-tabulation showing counts AND percentages by split
        #    - Verify ALL genders are represented and proportionally correct
        #    - Show exact counts before and after creating the plot
        #
        # 5. For tissue type or other categorical plots:
        #    - Use horizontal orientation for better readability when many categories exist
        #    - Ensure all category labels are readable
        #    - Group small categories as "Other" if there are too many to display clearly

        # IMPORTANT VISUALIZATION NAMING REQUIREMENTS:
        - Save all plots to the output directory (task_3_workdir)
        - In your final report, include a clearly labeled "Generated Plots:" section
        - List EACH visualization filename with the .png extension (e.g., age_distribution.png)
        - The critic will search for these EXACT filenames - do not change names after listing them
        - For each filename listed, include a brief description of what the plot shows
        - Example format:
          Generated Plots:
          1. age_distribution.png - Shows age distribution across splits
          2. gender_distribution.png - Shows gender balance in each split
          3. tissue_distribution.png - Shows tissue type representation
        - Verify all plot files exist and are readable before completing your report

        # ⚠️ CRITICAL CODE EXECUTION REQUIREMENTS ⚠️
        1. ONLY USE EXECUTABLE CODE BLOCKS: 
           - ALWAYS use ```bash code blocks for executable code
           - NEVER use ```plaintext, ```json, or any other non-executable format

        2. VALID CODE EXECUTOR FORMAT:
           ```bash
           #!/bin/bash
           eval "$(conda shell.bash hook)"
           conda activate sklearn-env
           PYTHONFAULTHANDLER=1 python - <<END
           import pandas as pd
           import matplotlib.pyplot as plt
           # Your actual Python code here
           END
           ```

        3. TOOL USAGE RULES:
           - DO NOT use multi_tool_use.parallel() or any similar patterns
           - Use tools ONE AT A TIME in separate messages
           - Always wait for each tool's response before using another tool

        4. FOR ANALYZE_PLOT AND SEARCH_DIRECTORY:
           - Use these tools directly in your regular message format
           - Example: analyze_plot("task_3_workdir/age_distribution.png")
           - NOT in a code block

        5. INVALID FORMATS (DO NOT USE):
           ```plaintext
           multi_tool_use.parallel({
             "tool_uses": [...]
           })
           ```
           OR
           ```json
           {
             "tool": "analyze_plot",
             "parameters": {...}
           }
           ```

        FAILURE TO FOLLOW THESE GUIDELINES WILL CAUSE CODE EXECUTION TO FAIL.

        Your report should be:
        - Well-documented with clear explanations
        - Include all relevant code and outputs
        - Highlight important patterns and insights
        - Note any technical challenges encountered
        - Confirm that all required output files have been created

        # CRITICAL COMPLETION PROTOCOL
        # To properly conclude your task:
        # 1. Verify all output files exist and are readable using code - YOU MUST RUN A FINAL CODE BLOCK FOR THIS
        # 2. Provide a concise summary of what was accomplished
        # 3. Include a "Generated Files" section listing all output files with their purposes
        # 4. Include a "Generated Plots" section listing all created visualizations
        # 5. Follow your system prompt guidelines for properly finalizing your report


        Remember to:
        - Follow best practices in scientific computing
        - Include appropriate error handling
        - Make code reusable and maintainable
        - Document all implementation choices
        
    subtask_2_revision:
      text: >
        # CURRENT WORKFLOW STEP: Task 3, Subtask 2 - Data Splitting Implementation (REVISION)

        # TEAM COMPOSITION:
        # - ML/Data Engineer: Implements the technical solutions in code based on the team's plans
        # - Code Executor: Executes the code and returns the results.

        # IMPORTANT: This is ITERATION {iteration} of this subtask. You are expected to address the feedback provided by Team A in their review.

        # ⚠️ CRITICAL REQUIREMENT: DATASET-LEVEL SPLITTING ⚠️
        # If your previous implementation did not properly split by dataset/study:
        #
        # IF MULTIPLE DATASETS EXIST:
        # 1. You MUST select MULTIPLE datasets for testing
        #
        # 2. Your test datasets should:
        #    - Collectively contain at least 33% of total samples, when possible
        #    - Represent the overall distribution of age, sex, and tissue
        #    - Have NO overlap with train/validation datasets
        #
        # IF ONLY ONE DATASET EXISTS:
        # 3. You should use appropriate stratification techniques
        #
        # 4. You MUST explicitly demonstrate the representativeness of your test set
        #    with clear visualizations and statistics

        As the ML/Data Engineer, you have received FEEDBACK on your previous implementation from Team A. Your task is to:

        1. Review Team A's feedback carefully and acknowledge each point
        2. Implement the requested revisions to the data splitting approach
        3. Address all the issues and suggestions raised by the review team
        4. Enhance your previous implementation based on their guidance
        5. Create a revised comprehensive report that includes:
           - Response to each feedback point
           - Updated code implementation with proper dataset-level splitting
           - Clear tables showing sample counts per dataset and per split
           - Visualizations comparing distributions across splits
           - Statistical tests comparing distributions across splits
           - Confirmation of updated output files

        # ⚠️ CRITICAL PLOT QUALITY REQUIREMENTS ⚠️
        # You MUST verify all visualizations for quality and accuracy:
        #
        # 1. READABILITY: All labels, legends, and text must be clearly visible and appropriately sized
        #
        # 2. ACCURACY: All data categories must be correctly represented with appropriate colors and proportions
        #
        # 3. VERIFICATION: For EACH plot:
        #    - Print a summary table showing counts and percentages that match what's in the plot
        #    - Explicitly verify the numbers match what's visually represented
        #    - If any category appears missing or incorrectly sized, recreate the plot
        #
        # 4. For gender/sex distribution plots specifically:
        #    - Always create a cross-tabulation showing counts AND percentages by split
        #    - Verify ALL genders are represented and proportionally correct
        #    - Show exact counts before and after creating the plot
        #
        # 5. For tissue type or other categorical plots:
        #    - Use horizontal orientation for better readability when many categories exist
        #    - Ensure all category labels are readable
        #    - Group small categories as "Other" if there are too many to display clearly
        #
        # 6. VISUAL INSPECTION: For every plot, ask yourself these questions:
        #    - "Does this plot clearly show what I intend it to show?"
        #    - "Are all categories represented and visible?"
        #    - "Would someone unfamiliar with this data understand the visualization?"
        #    - "Do the visual proportions match the numerical statistics?"
        #    If the answer to ANY question is "no," you MUST revise the plot

        # IMPORTANT VISUALIZATION NAMING REQUIREMENTS:
        - Save all plots to the output directory (task_3_workdir)
        - In your final report, include a clearly labeled "Generated Plots:" section
        - List EACH visualization filename with the .png extension (e.g., age_distribution.png)
        - The critic will search for these EXACT filenames - do not change names after listing them
        - For each filename listed, include a brief description of what the plot shows
        - Example format:
          Generated Plots:
          1. age_distribution.png - Shows age distribution across splits
          2. gender_distribution.png - Shows gender balance in each split
          3. tissue_distribution.png - Shows tissue type representation
        - Verify all plot files exist and are readable before completing your report

        # ⚠️ CRITICAL CODE EXECUTION REQUIREMENTS ⚠️
        1. ONLY USE EXECUTABLE CODE BLOCKS: 
           - ALWAYS use ```bash code blocks for executable code
           - NEVER use ```plaintext, ```json, or any other non-executable format

        2. VALID CODE EXECUTOR FORMAT:
           ```bash
           #!/bin/bash
           eval "$(conda shell.bash hook)"
           conda activate sklearn-env
           PYTHONFAULTHANDLER=1 python - <<END
           import pandas as pd
           import matplotlib.pyplot as plt
           # Your actual Python code here
           END
           ```

        3. TOOL USAGE RULES:
           - DO NOT use multi_tool_use.parallel() or any similar patterns
           - Use tools ONE AT A TIME in separate messages
           - Always wait for each tool's response before using another tool

        4. FOR ANALYZE_PLOT AND SEARCH_DIRECTORY:
           - Use these tools directly in your regular message format
           - Example: analyze_plot("task_3_workdir/age_distribution.png")
           - NOT in a code block

        5. INVALID FORMATS (DO NOT USE):
           ```plaintext
           multi_tool_use.parallel({
             "tool_uses": [...]
           })
           ```
           OR
           ```json
           {
             "tool": "analyze_plot",
             "parameters": {...}
           }
           ```

        FAILURE TO FOLLOW THESE GUIDELINES WILL CAUSE CODE EXECUTION TO FAIL.

        Your revised report should:
        - Begin with a clear acknowledgment of the feedback points you're addressing
        - Explicitly show your dataset selection strategy for the test set
        - Provide justification for why your test set is representative
        - Include sample counts and percentages for each split
        - Show distribution analysis of key variables across splits
        - Note any ongoing challenges or limitations
        - Confirm that all required output files have been created or updated

        # CRITICAL COMPLETION PROTOCOL
        # To properly conclude your task:
        # 1. Verify all output files exist and are readable using code - YOU MUST RUN A FINAL CODE BLOCK FOR THIS
        # 2. Provide a concise summary of what was accomplished
        # 3. Include a "Generated Files" section listing all output files with their purposes
        # 4. Include a "Generated Plots" section listing all created visualizations
        # 5. Follow your system prompt guidelines for properly finalizing your report

        Remember to:
        - Maintain high-quality code with appropriate comments
        - Continue following computational efficiency guidelines
        - Ensure all visualizations are properly saved and documented
        - Address ALL the feedback points from Team A
        - Provide a complete, standalone report that incorporates the revisions

        # ⚠️ CRITICAL REPORT FORMAT REQUIREMENT ⚠️
        - Your FINAL report message MUST directly include all tabular statistics in plaintext format
        - Do NOT just print statistics during code execution and omit them from your final message
        - Copy-paste all important statistics tables directly into your final report (in markdown format) before saying the termination phrase / token.
        - The critic can ONLY evaluate statistics that appear in your final message
        - Statistics printed during code execution but omitted from your final report will be IGNORED by the critic
        
    subtask_3:
      text: >
        # CURRENT WORKFLOW STEP: Task 3, Subtask 3 - Data Splitting Review (Team A Review)

        # TEAM COMPOSITION:
        # - Principal Scientist: Team leader who makes key research decisions and determines when discussions should conclude
        # - Bioinformatics Expert: Specialist in omics data processing with deep knowledge of DNA methylation data handling
        # - Machine Learning Expert: Provides insights on ML architectures, feature engineering, and model evaluation approaches

        Team A, you have received the implementation report from the ML/Data Engineer for the data splitting task. Your objective is to:

        1. Review the implementation and results thoroughly
        2. Assess whether the splits follow best practices for epigenetic datasets:
           - Dataset-level separation for test sets (if multiple datasets exist)
           - Appropriate stratification (for age, sex, and tissue type)
           - Proper verification of split quality
        3. Evaluate the evidence provided:
           - Check tabular statistics for distribution balance
           - Examine visualizations for clarity and information value
           - Verify that statistical tests confirm representativeness
        4. Determine if the implementation meets the critical requirements

        IMPORTANT: You should focus on these critical aspects:
        - The engineer MUST have properly separated datasets between test and train/val if multiple datasets exist
        - The splits should maintain similar distributions of age, sex, and tissue across all splits
        - There should be explicit statistical tests showing that splits are representative
        - Tabular statistics should clearly show the counts and percentages for key variables
        - Visualizations should effectively demonstrate the quality of the splits

        Based on your review, you should:
        - Acknowledge successful aspects of the implementation
        - Point out any issues with the data splitting approach
        - Identify any missing or inadequate evidence
        - Suggest specific improvements if needed
        - Consider the implications for downstream model training

        The Principal Scientist will summarize the review and either:
        - Accept the implementation by saying "APPROVE"
        - Request revisions if needed by saying "REVISE"

        Remember to:
        - Be thorough and specific in your feedback
        - Consider both statistical and biological validity
        - Focus on the most important issues rather than minor details
        - Make your feedback actionable for the engineer
        - Use the analyze_plot tool to examine any visualizations referenced in the report
        
  checklists:
    plot_quality: >
      # CRITICAL PLOT QUALITY CHECKLIST
      Before finalizing any data visualization, verify that each plot meets these requirements:

      1. READABILITY:
         - All axis labels are clearly visible and properly sized
         - Legend is readable and correctly represents the data
         - Title clearly explains what the plot shows
         - Text is not overlapping or cut off
         - Font sizes are consistent and appropriately sized

      2. ACCURACY:
         - Data is correctly represented (no missing categories)
         - Color scheme properly differentiates between categories
         - Proportions and distributions match the underlying data
         - No data points are accidentally excluded
         - Scales are appropriate for the data range

      3. INTERPRETABILITY:
         - The key insight is immediately apparent 
         - Comparisons between groups are clear
         - For crowded categorical plots:
           * Consider using horizontal orientation for long category names
           * Use proper spacing or faceting to avoid overcrowding
           * Ensure all categories are readable (rotate labels if needed)
         
      4. VISUAL VERIFICATION:
         - After creating each plot, you MUST:
           * Explicitly print summary statistics that confirm what the plot shows
           * Compare these statistics with what's visually represented
           * Check for any discrepancies between the numbers and the visualization
         
      5. QUALITY CONTROL STEPS:
         - For each visualization, run this verification code:
           * Print the exact count and percentage of items in each category
           * Verify these numbers match what's shown in the plot
           * If any category appears missing or incorrectly sized, recreate the plot 