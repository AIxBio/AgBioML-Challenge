description: Develop an epigenetic clock model that predicts chronological age from DNA methylation data
name: develop_epigenetic_clock

project_goal: |
  Develop an accurate epigenetic clock to predict chronological age from DNA methylation data, 
  with the aim of achieving state-of-the-art performance. 
  
  CRITICAL COMPLETION CRITERIA - The project MUST achieve BOTH of these criteria:
  1. Test set performance: Pearson correlation > 0.9 AND MAE < 10 years on average
  AND
  2. Dataset-level K-Fold Cross-Validation: Pearson correlation > 0.94 AND MAE < 6 years
     - CRITICAL: Folds MUST be created at the dataset level
     - No dataset can appear in both training and test sets within any single fold
     - Minimum of 5 folds required
  
  Both evaluation approaches MUST be performed and must meet their respective performance targets.
  The test set evaluation assesses generalization to unseen samples, while dataset-level K-Fold CV
  evaluates generalization to entirely new datasets, which is critical for real-world application.
  
  ADDITIONAL MANDATORY REQUIREMENTS:
  1. Final Model Preservation: The best-performing model MUST be saved as a checkpoint with an accompanying 
     inference script that can load and use the model for predictions on new data.
  2. Scientific Paper: Once the performance criteria are met, a complete scientific paper in markdown format 
     must be written documenting the research, methods, and results.
  3. Final Verification: The principal scientist must verify all requirements have been completed and 
     explicitly declare "ENTIRE_TASK_DONE" to conclude the project.

project_context: |
  DNA methylation patterns change with age and can be used as biomarkers to predict chronological age.
  This epigenetic clock project involves analyzing methylation data to build a regression model
  that accurately predicts age, which has applications in forensics, health assessment, and longevity research.

available_data:
  - name: betas.arrow
    description: DNA methylation beta values (feature matrix)
  - name: metadata.arrow
    description: Sample metadata including chronological ages (target variable)

data_completeness: |
  IMPORTANT: All data necessary to complete this task has been provided. The agents MUST work with the provided data files:
  - betas.arrow and metadata.arrow contain all required information
  - Sample IDs in both files match perfectly - there are NO missing mappings
  - No additional data files, ID mapping files, or external resources are needed or should be requested
  - The data is complete, properly formatted, and sufficient to achieve the performance criteria
  
  The agent MUST use these files as provided without requesting additional data. Any claims that the data is 
  incomplete, missing mappings, or insufficient are incorrect and should be ignored.

autonomous_workflow:
  approach: |
    This project follows a collaborative, autonomous workflow where agents determine the best course
    of action based on current project state rather than rigid predefined stages. Agents are expected to:
    1. Continuously assess project status through the lab notebook
    2. Identify gaps and opportunities in the current research
    3. Propose logical next steps based on scientific merit and feasibility
    4. Make decisions collaboratively when alternatives exist
    5. Document all decisions and their rationale, findings, observations, tips, and lessons learned in the lab notebook
  
  methodology: |
    The development process should generally include these components, though not necessarily in sequence:
    - Exploratory data analysis
    - Feature engineering and selection
    - Data preprocessing and normalization
    - Thoughtful data splits
    - Model selection and hyperparameter tuning
    - Performance evaluation with appropriate metrics
    - Error analysis and model improvement

  expected_outcomes:
    - Clear documentation of data exploration findings
    - Well-justified preprocessing and feature selection decisions
    - At least 2-3 model approaches with comparative analysis
    - Final model with validation and test performance metrics
    - Critical assessment of model strengths and limitations
    - Recommendations for future improvements
    
lab_notebook_guidelines: |
  The lab notebook serves as the central coordination mechanism for the project.
  Every entry should be dated and include:
  - Current project status assessment
  - Key decisions made and their rationale
  - Results of analyses or experiments
  - Next steps with clear rationale
  - Open questions or challenges
  
  The lab notebook should maintain sufficient context for any agent to understand
  the current state of the project, previous findings, and planned direction.

# The following tasks are kept as reference materials that the agents can adapt as needed
# Rather than being followed rigidly, they provide guidance for common workflow steps

reference:
  understanding:
    text: |
      # REFERENCE: Understanding the task
      
      Key questions to consider in understanding the epigenetic clock development task:
        1. What is the purpose of this task? How does it relate to the overall goals of the aging research field?
        2. What prior studies have been done on this type of task? What are the main approaches and methods used?
        3. What are the main challenges and limitations of the prior studies?
        4. What are the most promising approaches and methods for this task?
        5. What is the nature of the data? What normalizations and transformations should be considered?
        6. How should the data be explored or visualized? What are some typical QC approaches for it?

  eda:
    text: |
      # REFERENCE: Exploratory Data Analysis
      
      Key aspects to include in your EDA
      1. Data quality assessment
         - Missing data analysis
         - Distribution of beta values
         - Check for outliers and anomalies
      2. Sample characteristics
         - Age distribution 
         - Tissue type distribution
         - Sex distribution 
         - Other relevant metadata exploration
      3. Correlations and relationships
         - Correlations between probes
         - Relationships between age and beta values
         - Tissue-specific patterns
      4. Dimensionality and feature considerations
         - Total number of probes (features)
         - Potential for dimensionality reduction
         - Feature importance assessment methods
      
  data_splitting:
    text: |
      # REFERENCE: Data Splitting

        # CRITICAL BIOLOGICAL DATA SPLITTING GUIDELINES
        When splitting biological data, you MUST consider the following hierarchical sources of technical variance (from largest to smallest effect):

        1. Study/Dataset Level:
           - Studies represent a data collection effort, typically performed by a single research group with their own equipment, personnel, and protocols
           - Different studies have different protocols and equipment, which can introduce technical bias
           - Often a major source of technical variance that can lead to overfitting
           - Example: If your data contains 10 studies, you should split the data (or do LOOCV) by study when possible

        2. Organism/Donor Level:
           - Every human donor has a unique genome, lifestyle, and environmental exposures
           - Even inbred mice show biological variability
           - Data from the same donor should NEVER be split across train/val/test sets
           - All samples from a single donor must stay together in the same split

        3. Batch Level:
           - Represents groups of samples processed together using the same protocol and personnel (even if they are from different donors)
           - If batch information is available, consider stratifying splits by batch
           - Be careful not to confound batches with biological variables of interest

        4. Sample Level:
           - Samples with the same ID should never be split across datasets
           - Multiple measurements from the same sample must stay together in the same split

      # Critical priorities for splitting strategies:
      1. Tissue type representation (HIGHEST PRIORITY) - The test set MUST represent the tissue diversity
      2. Age distribution (SECOND PRIORITY) 
      3. Sex distribution (THIRD PRIORITY)
      
      # MANDATORY DATASET LEAKAGE VERIFICATION STEPS
      These steps MUST be followed - results are INVALID without these verifications:
      
      1. BEFORE SPLITTING:
         - Identify and document all unique datasets in your data
         - Create a mapping of each sample to its source dataset
      
      2. DURING SPLITTING:
         - Implement dataset-level splits (entire datasets go to either train, val (if there is one), or test, never more than one of these)
         - For K-Fold CV, ensure each fold maintains dataset-level separation
         - Document which datasets are assigned to which splits for each fold
      
      3. VERIFICATION (REQUIRED):
         - Implement explicit verification code that checks for ANY overlap between datasets in train/test
         - Print a summary showing which datasets appear in each split
         - Calculate and print intersection between train and test dataset lists (must be empty)
         - Include verification in EVERY script that performs evaluation
      
      4. DOCUMENTATION:
         - Record in the lab notebook exactly which datasets were used in each split
         - Include the verification results showing zero overlap
         - This documentation is MANDATORY before any evaluation results can be considered valid
      
      Example of verification for dataset leakage:
      ```python
      # Get unique dataset IDs in each split
      train_datasets = set(train_data['dataset_id'].unique())
      test_datasets = set(test_data['dataset_id'].unique())
      
      # Verify zero overlap
      overlap = train_datasets.intersection(test_datasets)
      if len(overlap) > 0:
          raise ValueError(f"CRITICAL ERROR: Dataset leakage detected! The following datasets appear in both train and test: {overlap}")
      else:
          print("✓ VERIFICATION PASSED: No dataset appears in both training and testing splits")
          
      # Document datasets in each split
      print(f"Training datasets ({len(train_datasets)}): {sorted(train_datasets)}")
      print(f"Testing datasets ({len(test_datasets)}): {sorted(test_datasets)}")
      ```
  
  evaluation:
    text: |
      # REFERENCE: Evaluation Script Development
      
      Key components to include in your evaluation script:
      1. Core metrics
         - Pearson correlation coefficient (primary metric)
         - Mean Absolute Error (MAE)
         - Root Mean Squared Error (RMSE) 
         - Additional metrics like R-squared
      2. Subgroup analysis
         - Performance breakdown by tissue type
         - Performance breakdown by age group
         - Performance breakdown by sex
      3. Visualizations
         - Scatter plot of predicted vs. true ages
         - Residual plots
         - Tissue-specific performance plots
      4. Implementation requirements
         - Command-line interface
         - Accept file paths for ground truth and predictions
         - Output metrics in both console and saved format
         - Generate visualizations automatically
  
  model_building:
    text: |
      # REFERENCE: Model Building
      
      Key considerations for model building:
      1. Model types to consider
         - Linear regression models (ElasticNet, Lasso, Ridge)
         - Tree-based models (Random Forest, Gradient Boosting)
         - Neural networks 
      2. Feature selection/engineering approaches
         - Variance-based filtering
         - Correlation with age
         - Domain knowledge about specific CpG sites
         - Dimensionality reduction techniques
      3. Script requirements
         - Training script that loads data, trains model, saves to output dir
         - Prediction script that loads model, generates predictions on new data
         - Clear documentation of hyperparameters
      4. Hyperparameter considerations
         - Regularization strength
         - Learning rates/schedules
         - Tree depth/number
         - Batch size (if applicable)
      
  training_evaluation:
    text: |
      # REFERENCE: Model Training and Evaluation
      
      Key steps in the final training and evaluation:
      1. Configuration decisions
         - Final hyperparameter selection
         - Data preprocessing finalization
         - Optimization settings
      2. Execution workflow
         - Full training on train set
         - Evaluation on validation set
         - Final evaluation on test set
         - Comprehensive metrics collection
      3. Analysis of results
         - Compare to baselines
         - Identify strengths and weaknesses
         - Subgroup performance analysis
         - Feature importance understanding
      
  iteration:
    text: |
      # REFERENCE: Review and Iteration
      
      Key aspects for effective model iteration:
      1. Analysis of current results
         - Identify specific areas of weakness
         - Understand error patterns
         - Compare to literature benchmarks
      2. Hypothesis generation
         - Develop theories about performance limitations
         - Review literature for potential improvements
         - Consider alternative approaches
      3. Implementation planning
         - Targeted modifications to improve weak areas
         - Experimental strategy for testing changes
         - Clear metrics to determine success
        
  checklists:
    plot_quality: |
      # CRITICAL PLOT QUALITY CHECKLIST
      Before finalizing any data visualization, verify that each plot meets these requirements:

      1. READABILITY:
         - All axis labels are clearly visible and properly sized
         - Legend is readable and correctly represents the data
         - Title clearly explains what the plot shows
         - Text is not overlapping or cut off
         - Font sizes are consistent and appropriately sized

      2. ACCURACY:
         - Data is correctly represented (no missing categories)
         - Color scheme properly differentiates between categories
         - Proportions and distributions match the underlying data
         - No data points are accidentally excluded
         - Scales are appropriate for the data range

      3. INTERPRETABILITY:
         - The key insight is immediately apparent 
         - Comparisons between groups are clear
         - For crowded categorical plots:
           * Consider using horizontal orientation for long category names
           * Use proper spacing or faceting to avoid overcrowding
           * Ensure all categories are readable (rotate labels if needed)
         
      4. VISUAL VERIFICATION:
         - After creating each plot, you MUST:
           * Explicitly print summary statistics that confirm what the plot shows
           * Compare these statistics with what's visually represented
           * Check for any discrepancies between the numbers and the visualization
         
      5. QUALITY CONTROL STEPS:
         - For each visualization, run this verification code:
           * Print the exact count and percentage of items in each category
           * Verify these numbers match what's shown in the plot
           * If any category appears missing or incorrectly sized, recreate the plot 

  scientific_paper:
    text: |
      # REFERENCE: Scientific Paper Requirements
      
      Once the performance criteria have been met, a comprehensive scientific paper must be written in markdown format.
      The paper should document the entire research process, methods, and findings. 
      
      ## Paper Structure Requirements
      The scientific paper MUST include these sections:
      1. **Title**: Descriptive and concise
      2. **Abstract**: Summary of the problem, approach, and key findings (200-300 words)
      3. **Introduction**: Background on epigenetic clocks, problem statement, and goals
      4. **Methods**:
         - Data description and preprocessing
         - Feature selection approach
         - Model architecture and development
         - Evaluation methodology
      5. **Results**:
         - Performance metrics with tables and figures
         - Comparison with baseline models
         - Subgroup analyses
      6. **Discussion**:
         - Interpretation of results
         - Comparison with existing literature
         - Limitations and strengths
      7. **Conclusion**: Key takeaways and future work
      8. **References**: Previous work cited in the paper
      
      ## Figures and Tables Requirements
      - All figures must be properly referenced in the text
      - Tables must be formatted using markdown syntax
      - Figures should be included as references to image files
      - Each figure must have a descriptive caption
      
      ## Paper Evaluation Rubric
      The scientific paper will be evaluated on these criteria:
      
      | Section (weight) | What the agent should verify | 0 | 1 | 2 | 3 |
      |------------------|------------------------------|---|---|---|---|
      | Abstract (5 %) | Clear problem statement, brief methods, principal quantitative results, single-sentence impact | Missing or vacuous | ≥1 element present but unclear | All elements present, minor clarity issues | ≤ 250 words, precise & compelling |
      | Introduction (10 %) | Concise background, gap identification, explicit objectives/hypotheses | No context or objectives | Context but gap/aims vague | Gap & aims stated; limited literature synthesis | Crisp synthesis of field; cites key work ≤ 5 yr; gap & aims crystal-clear |
      | Methods (20 %) | Data provenance, preprocessing, model architecture/hyper-params, evaluation protocol, software/versions | < 50 % info | ≥ 50 % but critical details missing | All details; small omissions | Fully reproducible; code repo link; power/sample-size justification |
      | Results (15 %) | Quantitative metrics, visualisations, statistical tests | No real results | Results w/o statistical rigor/plots | Stats & visuals adequate | Objectives mapped; error bars/CIs; ablation or baseline comparison |
      | Figures & Tables (5 %) | Readability, self-contained captions, correct units | Absent/illegible | Basic but poor labels/captions | Clear; captions ≈ 1–2 sentences | Publication-ready; colour-blind safe; axes & legends unambiguous |
      | Discussion & Conclusion (15 %) | Interpretation, relation to prior work, limitations, future directions | Superficial restatement | Some interpretation; no limits | Balanced interpretation & 1–2 limits | Deep synthesis; compares to SOTA; candid limits; concrete next steps |
      | Statistical & Computational Rigor (10 %) | Correct tests, multiple-comparison control, proper splits | Major flaws | Minor flaws | Appropriate tests & splits | Sensitivity analyses, robustness checks, effect sizes |
      | Model Checkpoints & Inference Script (10 %) | Availability of trained weights and script that reproduces paper metrics | None shared | Only script or checkpoint, not both | Both provided; minimal instructions | Turn-key script (CLI/README), version-tagged checkpoints, SHA/DOI, env spec |
      | Writing Quality & Style (5 %) | Grammar, flow, structure, terminology consistency | Hard to read | Understandable but clunky | Clear prose; few typos | Polished, concise, consistent voice |
      | References (5 %) | Completeness, formatting, recency | < 70 % cited | ≥ 70 % cited; poor format | Complete & consistent | DOIs/open-access links; ≥ 50 % refs ≤ 5 yr |
      
      To achieve a satisfactory evaluation, the paper should score at least 2 in each category and have an overall weighted average of at least 2.5.
      
      ## Self-Evaluation Requirements
      Before concluding the project, the principal scientist must perform a formal self-evaluation against this rubric.
      The self-evaluation must be presented in the following format:
      
      ```
      # PAPER SELF-EVALUATION
      
      ## Individual Category Assessments
      
      ### Abstract (5%) - Score: [X/3]
      [Detailed justification for score, referencing specific elements of the abstract]
      
      ### Introduction (10%) - Score: [X/3]
      [Detailed justification for score, referencing specific elements of the introduction]
      
      [Continue for all 10 categories with detailed justifications...]
      
      ## Weighted Score Calculation
      
      | Category | Weight | Score | Weighted Score |
      |----------|--------|-------|---------------|
      | Abstract | 5%     | X     | X * 0.05      |
      | Introduction | 10% | X    | X * 0.10      |
      | Methods   | 20%   | X     | X * 0.20      |
      | Results   | 15%   | X     | X * 0.15      |
      | Figures & Tables | 5% | X  | X * 0.05     |
      | Discussion & Conclusion | 15% | X | X * 0.15 |
      | Statistical & Computational Rigor | 10% | X | X * 0.10 |
      | Model Checkpoints & Inference Script | 10% | X | X * 0.10 |
      | Writing Quality & Style | 5% | X | X * 0.05 |
      | References | 5% | X | X * 0.05 |
      | **TOTAL** | **100%** | | **[Weighted Average]** |
      
      ## Final Assessment
      [Statement confirming weighted average > 2.5 and all categories ≥ 2]
      [Overall assessment of paper quality]
      [Statement confirming all other project requirements are met]
      ```
      
      Use the provided calculator tool to compute the weighted average score with this expression:
      
      ```
      calculator("sum([0.05*abstract_score, 0.10*intro_score, 0.20*methods_score, 0.15*results_score, 0.05*figures_score, 0.15*discussion_score, 0.10*stats_score, 0.10*model_score, 0.05*writing_score, 0.05*references_score])")
      ```
      
      IMPORTANT: Note the square brackets [ ] around the values. Aggregator functions like sum() require iterables.
      
      This self-evaluation must be completed and passed before the project can be considered complete.
  
  model_preservation:
    text: |
      # REFERENCE: Model Checkpoint and Inference Requirements
      
      The final model MUST be preserved with a usable inference pipeline. This requirement ensures reproducibility
      and practical application of the developed epigenetic clock.
      
      ## Model Saving Requirements
      The saved model must include:
      1. The trained model weights/parameters in an appropriate format (pickle, h5, pt, etc.)
      2. Any preprocessing transforms or scalers used in the pipeline
      3. Feature selection information (which CpG sites/features were used)
      4. Metadata about the model (performance metrics, training date, etc.)
      
      ## Inference Script Requirements
      An inference script must be provided that:
      1. Loads the saved model and associated components
      2. Accepts standardized input data (e.g., from a CSV/arrow file)
      3. Properly preprocesses the input data
      4. Generates age predictions
      5. Returns the predictions in a user-friendly format
      
      ## Documentation Requirements
      Documentation must include:
      1. Clear instructions on how to use the inference script
      2. Description of input data format requirements
      3. Explanation of the output format
      4. Example command for running inference
      5. Minimum hardware/software requirements
      
      ## Quality Assurance Requirements
      Before finalizing the model checkpoint and inference script:
      1. Verify that the model can be loaded successfully
      2. Test the inference script on sample data not used in training
      3. Confirm that the results match expected outputs
      4. Ensure all dependencies are documented 