# @package _global_
defaults:
  - override /agents: default
  - override /docker: default
  - override /hydra/launcher: joblib

# Multirun experiment: Compare model performance with/without public evaluation
max_iterations: 25

# Hydra multirun configuration with parallel execution
hydra:
  launcher:
    n_jobs: 4  # Number of parallel jobs - adjust based on your CPU cores
    batch_size: auto  # Batch size for job submission
    timeout: null  # No timeout for individual jobs
  sweep:
    dir: ./outputs/multirun_comparison_challenge1/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${model}_public-eval-${enable_public_evaluation}_run-${hydra.job.num}

# Usage examples (now with parallel execution):
# 1. All model/eval combinations with multiple runs (will run 4 jobs in parallel):
#    bioagents --multirun +experiment=multirun_comparison task_dir=challenges/01_basic_epigenetic_clock model=gpt-4.1,gpt-4o enable_public_evaluation=true,false +seed=0,1,2,3,4,5
#
# 2. Specific configuration (5 parallel runs):
#    bioagents --multirun +experiment=multirun_comparison task_dir=challenges/01_basic_epigenetic_clock model=gpt-4.1 enable_public_evaluation=true +seed=0,1,2,3,4
#
# 3. Adjust parallelism on command line:
#    bioagents --multirun +experiment=multirun_comparison task_dir=challenges/01_basic_epigenetic_clock model=gpt-4.1,gpt-4o enable_public_evaluation=true,false hydra.launcher.n_jobs=8 